{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52066614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (61.3.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (3.20.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "142ce882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:48:43.542180: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-07 16:48:43.542372: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from NLP_Natural_Disasters.data import get_data, clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f79cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a0e60",
   "metadata": {},
   "source": [
    "Load the dataframe train.csv cleaned :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8a69035",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_data(get_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48940142",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['target']\n",
    "X_train = train['text']\n",
    "X_train_with_id = train.drop(columns='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebe8419b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                               deed reason earthquake may allah forgive u\n",
       "1                                                    forest fire near la ronge sask canada\n",
       "2    resident asked shelter place notified officer evacuation shelter place order expected\n",
       "3                                      people receive wildfire evacuation order california\n",
       "4                                   got sent photo ruby alaska smoke wildfire pours school\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2de1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forest fire near la ronge sask canada'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de477df6",
   "metadata": {},
   "source": [
    "Later fine-tune : Spell check is computationally expensive : impouve data cleaning for the train : make better prediction ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ac9e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (2.6.1)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Please spelcheck this sentence'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install autocorrect\n",
    "!pip install autocorrect\n",
    "from autocorrect import Speller \n",
    "\n",
    "#create function to spell check strings\n",
    "def spell_check(x):\n",
    "    spell = Speller(lang='en')\n",
    "    return \" \".join([spell(i) for i in x.split()])\n",
    "\n",
    "#showcase spellcheck \n",
    "mispelled = 'Pleaze spelcheck this sentince'\n",
    "spell_check(mispelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8841b0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = spell_check(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277e3c4",
   "metadata": {},
   "source": [
    "Apply changes to test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3edbbf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('/home/dianavo/code/AxelCatelan/NLP_Natural_Disasters/raw_data/test.csv')\n",
    "X_test = clean_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8636547b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>earthquake safety los angeles ûò safety fastener xrwn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>storm ri worse last hurricane cityampothers hardest hit yard look like bombed around k still without power</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>cityofcalgary activated municipal emergency plan yycstorm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  \\\n",
       "3258  10861   \n",
       "3259  10865   \n",
       "3260  10868   \n",
       "3261  10874   \n",
       "3262  10875   \n",
       "\n",
       "                                                                                                            text  \n",
       "3258                                                      earthquake safety los angeles ûò safety fastener xrwn  \n",
       "3259  storm ri worse last hurricane cityampothers hardest hit yard look like bombed around k still without power  \n",
       "3260                                                                               green line derailment chicago  \n",
       "3261                                                                     meg issue hazardous weather outlook hwo  \n",
       "3262                                                   cityofcalgary activated municipal emergency plan yycstorm  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e469999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X_test = spell_check(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894440e0",
   "metadata": {},
   "source": [
    "Should be removed before predict on it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc39fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'uû÷ªòóåáåáåáï¡ì©ê'\n",
    "# delete if a character or 2 letters is repeated 3times : jesssssssee , tltltltl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "108b3eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15807 different words in your corpus\n"
     ]
    }
   ],
   "source": [
    "# tokenize the vocabulary \n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "vocab_size = len(tk.word_index)\n",
    "print(f'There are {vocab_size} different words in your corpus')\n",
    "X_token = tk.texts_to_sequences(X_train)\n",
    "\n",
    "# # Padding inputs : feed in the same tensor length\n",
    "# X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "    \n",
    "# X_pad, y_train, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550f861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "100204d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: python-Levenshtein in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: setuptools in /home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages (from python-Levenshtein) (61.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452905a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61840abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a word2vec embedding\n",
    "word2vec_transfer = api.load(\"glove-wiki-gigaword-50\")\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence_with_TF(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec:\n",
    "            embedded_sentence.append(word2vec[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence_with_TF(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed_2 = embedding(word2vec_transfer, X_train)\n",
    "X_test_embed_2 = embedding(word2vec_transfer, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed_2, dtype='float32', padding='post', maxlen=35)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed_2, dtype='float32', padding='post', maxlen=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf786fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 35, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad_2.shape\n",
    "# nb token,pad_sequence size : nb words , 50 : embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddc99ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR ??\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# word2vec = Word2Vec(sentences=X_train)\n",
    "# wv = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9bd43767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15807"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c1a841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 35, 50)            790400    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 35, 10)            2440      \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 10)                840       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                330       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 794,041\n",
      "Trainable params: 794,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 11.4 s, sys: 352 ms, total: 11.8 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# max length of char in a tweet\n",
    "maxlen=35\n",
    "embedding_size = 50\n",
    "\n",
    "def build_model_rnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(\n",
    "        input_dim= vocab_size+1,\n",
    "        input_length=maxlen,\n",
    "        output_dim=embedding_size,\n",
    "        mask_zero=True\n",
    "    ))\n",
    "    model.add(layers.LSTM(10, return_sequences=True))\n",
    "    model.add(layers.LSTM(10))\n",
    "    model.add(layers.Dense(30, activation='relu'))\n",
    "    model.add(layers.Dropout(0.15))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(learning_rate=0.0005), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_rnn = build_model_rnn()\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bce40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5094614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 35) for input KerasTensor(type_spec=TensorSpec(shape=(None, 35), dtype=tf.float32, name='embedding_3_input'), name='embedding_3_input', description=\"created by layer 'embedding_3_input'\"), but it was called on an input with incompatible shape (None, 35, 50).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_3\" (type Sequential).\n    \n    Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 35, 50, 50)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 35, 50), dtype=float32)\n      • training=True\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model_cnn \u001b[38;5;241m=\u001b[39m build_model_rnn()\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pad_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, history[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, history[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/dianavo/.pyenv/versions/3.8.12/envs/nlp_project/lib/python3.8/site-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_3\" (type Sequential).\n    \n    Input 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 35, 50, 50)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 35, 50), dtype=float32)\n      • training=True\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "model_cnn = build_model_rnn()\n",
    "\n",
    "history = model_cnn.fit(X_train_pad_2, y_train, \n",
    "          validation_split=0.3,\n",
    "          epochs=10, \n",
    "          batch_size=16,\n",
    "          callbacks=[es], verbose=1)\n",
    "print(\"Test Loss\", history[0])\n",
    "print(\"Test Accuracy\", history[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650a87bc",
   "metadata": {},
   "source": [
    "Prediction👌😍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff29a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) prediction\n",
    "y_new = model.predict(X_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
