{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f866fbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# from gensim.models.ldamulticore import LdaMulticore  # for big dataset, use multiple core machine\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words(\"english\")\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c20c8",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d6517aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4734b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLP_Natural_Disasters.data import get_data, clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59006e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### exemple Semi-suppervised LDA sur Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8bd37",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://www.kaggle.com/code/vanguarde/semi-supervised-lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb65137",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         texts_cleaning\u001b[38;5;241m.\u001b[39mappend(txt\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m texts_cleaning\n\u001b[0;32m---> 21\u001b[0m text \u001b[38;5;241m=\u001b[39m text_cleaning(\u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mtolist())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def text_cleaning(texts):\n",
    "    texts_cleaning = []\n",
    "    for txt in tqdm(texts):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        html = re.compile(r'<.*?>')\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        txt = emoji_pattern.sub(r'', txt)\n",
    "        txt = html.sub(r'',txt)\n",
    "        txt = url.sub(r'',txt)\n",
    "        txt = re.sub('[^A-Za-z\\s]', '', txt)\n",
    "        \n",
    "        texts_cleaning.append(txt.lower())\n",
    "    return texts_cleaning\n",
    "\n",
    "text = text_cleaning(train_df.text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaee6d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "text = [t.split() for t in text]\n",
    "stemmed_text = []\n",
    "ps = PorterStemmer()\n",
    "for sentence in tqdm(text):\n",
    "    sent = []\n",
    "    for word in sentence:\n",
    "        sent.append(ps.stem(word))\n",
    "    stemmed_text.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83119537",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(*stemmed_text[5][:20])\n",
    "print(*text[5][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438d8a5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(text_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24759b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "IF doesn't work : dictionary.doc2bow  --> tqdm\n",
    "tried solution :\n",
    "conda install -c conda-forge ipywidgets\n",
    "jupyter nbextension enable --py widgetsnbextension\n",
    "SOLUTION : remove 'in tqdm' add a splitted list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e0363",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stopword_ids = map(dictionary.token2id.get, stops)\n",
    "dictionary.filter_tokens(bad_ids=stopword_ids)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.7, keep_n=None)\n",
    "dictionary.compactify() # remove gaps in id sequence\n",
    "bow = [dictionary.doc2bow(line) for line in text_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f92c42",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "disasters = ['disaster', 'bloodbath', 'collapse', 'crash', 'meltdown', 'doomsday', 'convulsion', 'accident', 'casualty', 'fatality', \n",
    "            'blast', 'catastrophe', 'traffic','hybrid', 'engine', 'license', \n",
    "            'tsunami', 'volcano','tornado','avalanche','earthquake','blizzard','drought','bushfire','tremor','magma','twister',\n",
    "            'windstorm','cyclone','flood','fire','hailstorm','lava','lightning','hail','hurricane','seismic','erosion','whirlpool','whirlwind',\n",
    "            'cloud','thunderstorm','barometer','gale','blackout','gust','force','volt','snowstorm','rainstorm','storm','nimbus','violent storm',\n",
    "            'sandstorm','fatal','cumulonimbus','death','lost','destruction','money','tension','cataclysm','damage','uproot','underground',\n",
    "            'destroy','arsonist','wind scale','arson','rescue','permafrost','fault','shelter', 'bomb', 'suicide', 'tragedy', 'weapon']\n",
    "\n",
    "disasters = [ps.stem(word) for word in disasters]\n",
    "\n",
    "# add more topics later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1004f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seed_topics = {}\n",
    "for word in disasters:\n",
    "    seed_topics[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e1a1e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_eta(priors, etadict, ntopics):\n",
    "    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    for word, topic in priors.items(): # for each word in the list of priors\n",
    "        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n",
    "        if (len(keyindex)>0): # if it's in the dictionary\n",
    "            eta[topic,keyindex[0]] = 1e7  # put a large number in there\n",
    "    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cd162",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eta = create_eta(seed_topics, dictionary, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe32d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746c3e4d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For now, Number of topics = 2: disasters , common topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bac7c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random_state=42\n",
    "lda_model_core = LdaMulticore(corpus=bow, #bag of words\n",
    "                         id2word=dictionary, #our common dict, need for print words in topics, not numbers from bow\n",
    "                         num_topics=2,\n",
    "                         eta=eta, #our eta matrix\n",
    "                         chunksize=2000,\n",
    "                         passes=10,\n",
    "                         random_state=42,\n",
    "                         alpha='symmetric', #param of LDA distribution. If you dont know use symmetric\n",
    "                         per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56994ed2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for num, params in lda_model_core.print_topics():\n",
    "    print(f'{num}: {params}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e285633",
   "metadata": {},
   "source": [
    "### LDA Topic Modelisation avec Gensim :  Creer des topics (pertinent!) , analyse mot -> classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360289d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>forest fire near la canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>resident asked shelter place officer evacuatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>got sent photo alaska smoke wildfire school</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target\n",
       "0   1         deed reason earthquake may allah forgive u       1\n",
       "1   4                         forest fire near la canada       1\n",
       "2   5  resident asked shelter place officer evacuatio...       1\n",
       "3   6  people receive wildfire evacuation order calif...       1\n",
       "4   7        got sent photo alaska smoke wildfire school       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EDA cleaning process\n",
    "\n",
    "data = get_data()\n",
    "data_clean = clean_data(data)\n",
    "\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f3dea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_clean[data_clean['target']==1]\n",
    "X_train = X_train['text']\n",
    "\n",
    "# transform to a list, split to get list of each sentence splitted for gensim.corpora.Dict\n",
    "list_tweet = X_train.tolist()\n",
    "text_split = [x.split() for x in list_tweet]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abad03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation to create a Bag of Words .doc2bow for LdaModel later\n",
    "# stopword_ids = map(dictionary.token2id.get, stops)\n",
    "# dictionary.filter_tokens(bad_ids=stopword_ids)\n",
    "# dictionary.filter_extremes(no_below=20, no_above=0.7, keep_n=None)\n",
    "# dictionary.compactify() # remove gaps in id sequence\n",
    "bow = [dictionary.doc2bow(line) for line in text_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dc08ad7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bow_dict.joblib']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(dictionary, 'bow_dict.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c328f6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Analysis & Research - crash test !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a2dbc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Topic analysis : Raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798dfb80",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target_1_raw = data[data['target']==1]\n",
    "target_1_raw = target_1_raw['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6fdc5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(target_1_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbedf8e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Gensim LDA model syntax tuto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cddc63",
   "metadata": {
    "hidden": true
   },
   "source": [
    "https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b503f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Train an LDA model using a Gensim corpus\"\"\"\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(common_texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
    "\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=10)\n",
    "\n",
    "\n",
    "    \"\"\"Save a model to disk, or reload a pre-trained model\"\"\"\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)\n",
    "\n",
    "\n",
    "    \"\"\"Query, the model using new, unseen documents\"\"\"\n",
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [\n",
    "    ['computer', 'time', 'graph'],\n",
    "    ['survey', 'response', 'eps'],\n",
    "    ['human', 'system', 'computer']\n",
    "]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[0]\n",
    "vector = lda[unseen_doc]  # get topic probability distribution for a document\n",
    "\n",
    "\n",
    "    \"\"\"Update the model by incrementally training on the new corpus\"\"\"\n",
    "lda.update(other_corpus)\n",
    "vector = lda[unseen_doc]\n",
    "\n",
    "\n",
    "    \"\"\"A lot of parameters can be tuned to optimize training for your specific case\"\"\"\n",
    "lda = LdaModel(common_corpus, num_topics=50, alpha='auto', eval_every=5)  # learn asymmetric alpha from data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5099d555",
   "metadata": {},
   "source": [
    "### Create my own topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821158fc",
   "metadata": {},
   "source": [
    "> countvectoriser most_commun : tableau avec le compte totalde chaque mot et son occurence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24893e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to create my 3 topic, 3rd is random other subjects\n",
    "\n",
    "# this list means need forces intervention or involved\n",
    "forces_intervention = ['accident', 'officers', 'evacuation', 'shelter', 'wildfire', 'wildfires', 'smoke', 'fire', 'emergency', 'die', 'died' 'ablaze', 'burn', 'burned',\n",
    "'crash', 'accident', 'traffic', 'damage', 'deadly', 'kill', 'killed', 'explosion', 'death', 'ambulance', 'drunk', 'crime', 'attack', 'attacked', 'victim', 'stabbed', 'stabbing', 'arson',\n",
    "'arrested', 'arrestation', 'burning', 'arsonist', 'collided', 'collision', 'police', 'suicide', 'bombing', 'injured', 'terrorist', 'avalanche', 'army', 'derailed', 'stab', 'blew', 'collapse']\n",
    "\n",
    "weather_phenomen = ['eruption', 'earthquake', 'catastrophic effect','heavy', 'rain', 'flood', 'tornado', 'heat', 'tsunami', 'volcano','tornado', 'magma', 'windstorm','cyclone','hailstorm','lava','hail','hurricane','seismic','erosion',\n",
    "'whirlwind', 'cloud','thunderstorm','barometer','gale','blackout','gust','snowstorm','rainstorm','storm','nimbus','violent storm', 'sandstorm','cumulonimbus']\n",
    "\n",
    "# ps = PorterStemmer()\n",
    "lemmatizer  = WordNetLemmatizer()\n",
    "forces_intervention = [lemmatizer.lemmatize(word) for word in forces_intervention]\n",
    "weather_phenomen = [lemmatizer.lemmatize(word) for word in weather_phenomen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec8565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## just to see, take a word at its root : killed -> kill\n",
    "# forces_intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b43d8ed",
   "metadata": {},
   "source": [
    "**Placé dans target1**\n",
    "'star wars',  'Indeed!! I am fully aware of that battle! I support you in that fight!'\n",
    "\n",
    "menace : 'you can stab me in the back but I promise you'll be the one bleeding'\n",
    "\n",
    "news : 'transport bioterror germs in wake of anthrax'\n",
    "\n",
    "suspect = ['afraid', 'hate', 'bomb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8470f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize value 0 to each word to go through\n",
    "# then we'll use create_eta() function to add a big value if word in lists above\n",
    "\n",
    "seed_topics_forces = {}\n",
    "\n",
    "for word in forces_intervention:\n",
    "    seed_topics_forces[word] = 0\n",
    "    \n",
    "seed_topics_weather = {}\n",
    "\n",
    "for word in weather_phenomen:\n",
    "    seed_topics_weather[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcf2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to see\n",
    "# seed_topics_forces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f3f1e",
   "metadata": {},
   "source": [
    "Custom **eta** parameter of gensim.models.ldamodel.LdaModel (not sklrean ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b75d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eta(dict_priors, etadict, ntopics):\n",
    "    \"\"\" create 'eta' parameter in LdaModel,\n",
    "    dict_priors  is a list of dicts from see_topics \"\"\"\n",
    "    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    \n",
    "    for dict_prior in dict_priors:\n",
    "        \n",
    "        for word, topic in dict_prior.items(): # for each word in the list of priors\n",
    "            \n",
    "            keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n",
    "            if (len(keyindex)>0): # if it's in the dictionary\n",
    "                eta[topic,keyindex[0]] = 1e7  # put a large number in there\n",
    "                \n",
    "    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fb7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create my own eta, affect a big value to a word in my seed_topics_force, seed_topic_weather\n",
    "# IF is in my lists created above (force_intervention, weather_phenomen')\n",
    "eta = create_eta([seed_topics_forces, seed_topics_weather], dictionary, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad49aaf",
   "metadata": {},
   "source": [
    "Define LdModel() : gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58166c08",
   "metadata": {},
   "source": [
    "### LDA model custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90da90",
   "metadata": {},
   "source": [
    "class **gensim.models.ldamodel.LdaModel**(corpus=None, num_topics=100, id2word=None, distributed=False, chunksize=2000, passes=1, update_every=1, alpha='symmetric', eta=None, decay=0.5, offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001, minimum_probability=0.01, random_state=None, ns_conf=None, minimum_phi_value=0.01, per_word_topics=False, callbacks=None, dtype=<class 'numpy.float32'>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7430dc",
   "metadata": {},
   "source": [
    "**random_state** ({np.random.RandomState, int}, optional) – Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
    "random_state - this serves as a seed (in case you wanted to repeat exactly the training process)\n",
    "**chunksize** - number of documents to consider at once (affects the memory consumption)\n",
    "\n",
    "**update_every** - update the model every update_every chunksize chunks (essentially, this is for memory consumption optimization)\n",
    "\n",
    "**passes** - how many times the algorithm is supposed to pass over the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d53a56",
   "metadata": {},
   "source": [
    "chunksize - number of documents to consider at once (affects the memory consumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "415f505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=3\n",
    "chunksize=2000\n",
    "passes = 40\n",
    "random_state = 100\n",
    "\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow, #bag of words\n",
    "                         id2word=dictionary, #our common dict, need for print words in topics, not numbers from bow\n",
    "                         num_topics=num_topics,\n",
    "                         eta=eta, #our eta matrix\n",
    "                         chunksize=chunksize,\n",
    "                        update_every = 1,\n",
    "                         passes=passes,\n",
    "                         random_state=random_state,\n",
    "                         alpha ='asymmetric', #param of LDA distribution. If you dont know use symmetric\n",
    "                        minimum_probability = 0.0,\n",
    "                         per_word_topics=True,\n",
    "                        dtype =np.float32,\n",
    "                        iterations = 400,\n",
    "                        eval_every = None)\n",
    "\n",
    "# alpha ='symmetric' return SAME percentage, auto/asymmetric : return diff results, \n",
    "# random_state : serves as a seed (in case you wanted to repeat exactly the training process)\n",
    "# n_jobs = -1 ?\n",
    "# random_state=100,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "37e9e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"/home/dianavo/code/AxelCatelan/NLP_Natural_Disasters/model_lda\")\n",
    "\n",
    "lda_model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94beef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d575cc86",
   "metadata": {},
   "source": [
    "(!) alpha ='symmetric' return SAME percentage --> set to auto/asymmetric : return diff % dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb92fcc",
   "metadata": {},
   "source": [
    "the corpus contains the word id and its frequency in every document. We can create a BoW corpus from a simple list of documents and from text files. What we need to do is, to pass the tokenised list of words to the object named Dictionary. doc2bow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "631be990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.010*\"fire\" + 0.008*\"news\" + 0.007*\"california\" + 0.007*\"suicide\" + 0.006*\"family\" + 0.006*\"via\" + 0.006*\"crash\" + 0.005*\"pm\" + 0.005*\"home\" + 0.005*\"killed\" + 0.005*\"wildfire\" + 0.005*\"police\" + 0.004*\"video\" + 0.004*\"two\" + 0.004*\"legionnaire\" + 0.004*\"accident\" + 0.004*\"mh\" + 0.004*\"bomber\" + 0.004*\"people\" + 0.003*\"emergency\" + 0.003*\"kill\" + 0.003*\"get\" + 0.003*\"attack\" + 0.003*\"flood\" + 0.003*\"warning\"\n",
      "\n",
      "1: 0.010*\"fire\" + 0.009*\"hiroshima\" + 0.007*\"like\" + 0.007*\"u\" + 0.007*\"amp\" + 0.007*\"year\" + 0.006*\"train\" + 0.006*\"storm\" + 0.006*\"building\" + 0.006*\"atomic\" + 0.005*\"japan\" + 0.005*\"still\" + 0.005*\"im\" + 0.005*\"today\" + 0.005*\"life\" + 0.005*\"people\" + 0.004*\"bomb\" + 0.004*\"look\" + 0.004*\"bombing\" + 0.004*\"first\" + 0.004*\"mass\" + 0.004*\"day\" + 0.004*\"burning\" + 0.004*\"derailment\" + 0.004*\"war\"\n",
      "\n",
      "2: 0.020*\"disaster\" + 0.011*\"obama\" + 0.009*\"oil\" + 0.008*\"may\" + 0.008*\"spill\" + 0.007*\"u\" + 0.007*\"charged\" + 0.007*\"boy\" + 0.006*\"nuclear\" + 0.005*\"sign\" + 0.005*\"declares\" + 0.005*\"typhoondevastated\" + 0.005*\"saipan\" + 0.005*\"yearold\" + 0.005*\"missing\" + 0.005*\"projected\" + 0.005*\"memory\" + 0.005*\"la\" + 0.005*\"bigger\" + 0.005*\"manslaughter\" + 0.004*\"costlier\" + 0.004*\"refugio\" + 0.004*\"woman\" + 0.004*\"report\" + 0.004*\"hazardous\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print 10 words and its percentage in each topics (0-2)\n",
    "\n",
    "# print_topics(num_topics=20, num_words=10)\n",
    "for num, params in lda_model.print_topics(num_words=25):\n",
    "    print(f'{num}: {params}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "## afficher plus de mots pour chaque topic\n",
    "## pq que des 1, verif autres codes, frequences? param, get_term_topics()   pq : (78, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43d48c7",
   "metadata": {},
   "source": [
    "**First try : FAILED ! code deleted :(**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54585767",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Visualizing the entity recongnizer¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788d104",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# 1. Loading the language library\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = u'''\n",
    "Elon Musk, the billionaire CEO of Tesla and SpaceX, is now the richest person in the world, surpassing former titleholder and Amazon chief Jeff Bezos with a net worth of $189.7 billion, according to Forbes’s real-time billionaire net-worth estimates on Jan. 8, 2021 at 1pm.\n",
    "2 - Since March, Musk’s wealth has grown almost seven-fold, up a staggering $163.1 billion.\n",
    "'''\n",
    "# 2. Building a Pipline Object\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d41c506",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='dep', jupyter=True, options={'distance':90})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee28feb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5394c6b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Autre plot fancy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e25e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 3\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "data_samples = list_tweet\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=n_features, stop_words=\"english\"\n",
    ")\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\n\" * 2,\n",
    "    \"Fitting LDA models with tf features, n_samples=%d and n_features=%d...\"\n",
    "    % (n_samples, n_features),\n",
    ")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tf_feature_names = tf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tf_feature_names, n_top_words, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7cae3",
   "metadata": {},
   "source": [
    "### Get_document_topics :    Exemple with an input test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f5a9a",
   "metadata": {},
   "source": [
    "Objectif : comprendre \"parfaitement\" gensim.ldamodel (not sklearn Lda)\n",
    "\n",
    "1) faire marcher la pred >> voir le topic utilise les bon mots >> sinon regler **alpha** pour la distribution des topics \n",
    "\n",
    "Objectif 2 (short on time) Mieux classer : \n",
    "\n",
    "2) countvectoriser >> determiner l'occurence des mot en pair >> les mettre dans mes list de topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f7202",
   "metadata": {},
   "source": [
    "> **get_document_topics** : Get the most relevant topics to the given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cbd6174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41, 1), (77, 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also transform my test phrase\n",
    "dictionary.doc2bow(['airplane','crash'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1a81b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def countvecto(allsentences):\n",
    "#     \"\"\"Vectorise sentences, return an array\"\"\"\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     # tokenize and build vocab    \n",
    "#     X_test = vectorizer.transform(allsentences)\n",
    "#     return X_test\n",
    "#     # old ver : return X_test.toarray()\n",
    "\n",
    "def split_sentence(sentence):\n",
    "    \"\"\"split ONE sentence in individual words\"\"\"\n",
    "    sentence_spl = [x.split() for x in sentence]\n",
    "    return sentence_spl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a584703",
   "metadata": {},
   "source": [
    "**Re-run an lda model** to fit only on the sentence test, not the entire data train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd97d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_topic(splitted_sentences):\n",
    "#     \"\"\"Predict the corresponding topic with a list of sentences cleaned (stopwords) and splitted\"\"\"\n",
    "#     dictionary_pred = corpora.Dictionary(splitted_sentences)\n",
    "#     # dictionary.filter_extremes(no_below=20, no_above=0.7, keep_n=None)\n",
    "#     bow_pred = dictionary_pred.doc2bow(splitted_sentences)\n",
    "\n",
    "#     # old ver : predict_topic = lda_model.get_document_topics(bow_pred)\n",
    "#     predict_topic = lda_model_exemple.get_document_topics(bow_pred)\n",
    "    \n",
    "#     return predict_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b74664",
   "metadata": {},
   "source": [
    "> steps : lower, tokenize, doc2bow, get_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "504ee0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT an exemple as a list of ONE + CLEAN sentence ['man put fire school']\n",
    "exemple = ['horrible accident man died in wings of airplane eruption']\n",
    "#exemple = ['storm heavy rain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4c02e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_exemple_splitted = split_sentence(exemple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a111f4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['horrible',\n",
       "  'accident',\n",
       "  'man',\n",
       "  'died',\n",
       "  'in',\n",
       "  'wings',\n",
       "  'of',\n",
       "  'airplane',\n",
       "  'eruption']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_exemple_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13496194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(62, 1), (118, 1), (148, 1), (184, 1), (246, 1), (1563, 1)]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also transform my test phrase\n",
    "exemple_bow = [dictionary.doc2bow(line) for line in my_exemple_splitted]\n",
    "exemple_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cfda2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the function countvecto() to vectorize the sentence, fit in lda_model (Not fit_transform)\n",
    "# exemple_vecto = countvecto(my_exemple_splitted[0])  ### OR exemple ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d143aa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.9241476), (1, 0.04419785), (2, 0.03165454)]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the probability in each topic using predict_topic()\n",
    "predict_topic = lda_model.get_document_topics(exemple_bow)\n",
    "pourcentage = [element for element in predict_topic]\n",
    "pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da442307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.9241405), (1, 0.04420494), (2, 0.031654544)]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test model saved\n",
    "predict_topic = model_test.get_document_topics(exemple_bow)\n",
    "pourcentage = [element for element in predict_topic]\n",
    "pourcentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed63e98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = lda_model[exemple_bow]\n",
    "a = [element for element in test_predict]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "50950438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 1 Forces intervention, need or involved : [(0, 0.7957197), (1, 0.024716612), (2, 0.17956367)] %\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic 1 Forces intervention, need or involved : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpourcentage[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic 2 Weather phenomen : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpourcentage[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic 3 Rlandom topics : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpourcentage[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(f\"topic 1 Forces intervention, need or involved : {pourcentage[0]} %\")\n",
    "print(f\"topic 2 Weather phenomen : {pourcentage[1]} %\")\n",
    "print(f\"topic 3 Rlandom topics : {pourcentage[2]} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99326d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To plot at Jupyter notebook\n",
    "# pyLDAvis.enable_notebook()\n",
    "# plot = pyLDAvis.prepare(lda_model, bow, dictionary, vocab=)\n",
    "# # Save pyLDA plot as html file\n",
    "# pyLDAvis.save_html(plot, 'LDA_NYT.html')\n",
    "# plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3555c8",
   "metadata": {},
   "source": [
    "> Coherence value : Choosing the Best Number of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15299054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_coherence_values(dictionary, corpus, texts, \n",
    "#                              cohere, limit, start=2, step=2):\n",
    "#     iterations = 100\n",
    "#     temp = dictionary[0]\n",
    "#     id2word = dictionary.id2token\n",
    "\n",
    "#     coherence_values = []\n",
    "\n",
    "#     for num_topics in range(start, limit, step):\n",
    "#         model = LdaModel(corpus=corpus, \n",
    "#                          id2word=dictionary, \n",
    "#                          num_topics=num_topics,\n",
    "#                          chunksize=chunksize,\n",
    "#                          alpha='auto',\n",
    "#                          eta='auto',\n",
    "#                          iterations=iterations,\n",
    "#                          passes=passes,\n",
    "#                          eval_every=1,\n",
    "#                          random_state=42,)\n",
    "#         coherencemodel = CoherenceModel(model=model, \n",
    "#                                         texts=texts, \n",
    "#                                         dictionary=dictionary, \n",
    "#                                         coherence=cohere)\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "#     return coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 30 min to run, in 8G RAM ####\n",
    "\n",
    "# limit=50\n",
    "# start=2\n",
    "# step=2\n",
    "# corpus = bow\n",
    "# texts = text_split\n",
    "\n",
    "# coherence_values = compute_coherence_values(dictionary=dictionary, \n",
    "#                                             corpus=corpus, \n",
    "#                                             texts=texts, \n",
    "#                                             cohere='c_v', \n",
    "#                                             start=start, \n",
    "#                                             limit=limit, \n",
    "#                                             step=step)\n",
    "\n",
    "# plt.figure(figsize=(8,5))\n",
    "\n",
    "# # Create a custom x-axis\n",
    "# x = range(start, limit, step)\n",
    "\n",
    "# # Build the line plot\n",
    "# ax = sns.lineplot(x=x, y=coherence_values, color='#238C8C')\n",
    "\n",
    "# # Set titles and labels\n",
    "# plt.title(\"Best Number of Topics for LDA Model\")\n",
    "# plt.xlabel(\"Num Topics\")\n",
    "# plt.ylabel(\"Coherence score\")\n",
    "# plt.xlim(start, limit)\n",
    "# plt.xticks(range(2, limit, step))\n",
    "\n",
    "# # Add a vertical line to show the optimum number of topics\n",
    "# plt.axvline(x[np.argmax(coherence_values)], \n",
    "#             color='#F26457', linestyle='--')\n",
    "\n",
    "# # Draw a custom legend\n",
    "# legend_elements = [Line2D([0], [0], color='#238C8C', \n",
    "#                           ls='-', label='Coherence Value (c_v)'),\n",
    "#                    Line2D([0], [1], color='#F26457', \n",
    "#                           ls='--', label='Optimal Number of Topics')]\n",
    "\n",
    "# ax.legend(handles=legend_elements, loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0764e669",
   "metadata": {},
   "source": [
    "Display topics visually"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
